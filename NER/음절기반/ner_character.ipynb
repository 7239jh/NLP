{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'KoCharElectraTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'KoCharElectraTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, RandomSampler\n",
    "\n",
    "from utils import get_labels\n",
    "from KoCharELECTRA_master.tokenization_kocharelectra import KoCharElectraTokenizer\n",
    "tokenizer = KoCharElectraTokenizer.from_pretrained('monologg/kocharelectra-base-discriminator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 text, label 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts: ['오늘 삼성전자 주가는 얼마야', '두산에너빌리티 현황을 알려줘', '오늘 급등한 종목은 뭐야', '지금 LG전자 주가는 얼마야', '현재 삼성물산 주가는 얼마야', '오늘 한국카본 주가는 얼마야', '어제 LG화학 종가는 얼마니', '오늘 옵티시스 주가는 얼마야', '어제 대동기어 주가는 얼마야', '한화솔루션 가격을 알려줘']\n",
      "labels: [['DAT-B', 'DAT-I', 'O', 'ST-B', 'ST-I', 'ST-I', 'ST-I', 'O', 'PR-B', 'PR-I', 'O', 'O', 'O', 'O', 'O'], ['ST-B', 'ST-I', 'ST-I', 'ST-I', 'ST-I', 'ST-I', 'ST-I', 'O', 'PR-B', 'PR-I', 'O', 'O', 'O', 'O', 'O'], ['DAT-B', 'DAT-I', 'O', 'O', 'O', 'O', 'O', 'ST-B', 'ST-I', 'O', 'O', 'O', 'O'], ['DAT-B', 'DAT-I', 'O', 'ST-B', 'ST-I', 'ST-I', 'ST-I', 'O', 'PR-B', 'PR-I', 'O', 'O', 'O', 'O', 'O'], ['DAT-B', 'DAT-I', 'O', 'ST-B', 'ST-I', 'ST-I', 'ST-I', 'O', 'PR-B', 'PR-I', 'O', 'O', 'O', 'O', 'O'], ['DAT-B', 'DAT-I', 'O', 'ST-B', 'ST-I', 'ST-I', 'ST-I', 'O', 'PR-B', 'PR-I', 'O', 'O', 'O', 'O', 'O'], ['DAT-B', 'DAT-I', 'O', 'ST-B', 'ST-I', 'ST-I', 'ST-I', 'O', 'PR-B', 'PR-I', 'O', 'O', 'O', 'O', 'O'], ['DAT-B', 'DAT-I', 'O', 'ST-B', 'ST-I', 'ST-I', 'ST-I', 'O', 'PR-B', 'PR-I', 'O', 'O', 'O', 'O', 'O'], ['DAT-B', 'DAT-I', 'O', 'ST-B', 'ST-I', 'ST-I', 'ST-I', 'O', 'PR-B', 'PR-I', 'O', 'O', 'O', 'O', 'O'], ['ST-B', 'ST-I', 'ST-I', 'ST-I', 'ST-I', 'O', 'PR-B', 'PR-I', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "def df(path,name):\n",
    "    texts = []\n",
    "    labels=[]\n",
    "    with open(os.path.join(path, name), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            text, label = line.split('\\t')\n",
    "            label=label.split()\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "    return texts , labels\n",
    "print('texts:',df('./data','train.tsv')[0][:10])\n",
    "print('labels:',df('./data','train.tsv')[1][:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음절별로 id, label 붙히기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>오</td>\n",
       "      <td>DAT-B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>늘</td>\n",
       "      <td>DAT-I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>삼</td>\n",
       "      <td>ST-B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>성</td>\n",
       "      <td>ST-I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>3385</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>3386</td>\n",
       "      <td>시</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>3387</td>\n",
       "      <td>작</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3388</th>\n",
       "      <td>3388</td>\n",
       "      <td>인</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>3389</td>\n",
       "      <td>데</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3390 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id text  label\n",
       "0        0    오  DAT-B\n",
       "1        1    늘  DAT-I\n",
       "2        2           O\n",
       "3        3    삼   ST-B\n",
       "4        4    성   ST-I\n",
       "...    ...  ...    ...\n",
       "3385  3385           O\n",
       "3386  3386    시      O\n",
       "3387  3387    작      O\n",
       "3388  3388    인      O\n",
       "3389  3389    데      O\n",
       "\n",
       "[3390 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=df('./data','train.tsv')[0]\n",
    "df_t=[]\n",
    "for i in df_train:\n",
    "    df_t += i\n",
    "df_label=df('./data','train.tsv')[1]\n",
    "df_l=[]\n",
    "for i in df_label:\n",
    "    df_l += i\n",
    "import pandas as pd\n",
    "id=[]\n",
    "for i in range(len(df_l)):\n",
    "    id.append(i)\n",
    "df_sum=pd.DataFrame({'id':id,'text':df_t,'label':df_l})\n",
    "df_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오', 'DAT-B']\n",
      "['늘', 'DAT-I']\n",
      "[' ', 'O']\n",
      "['삼', 'ST-B']\n",
      "['성', 'ST-I']\n",
      "['전', 'ST-I']\n",
      "['자', 'ST-I']\n",
      "[' ', 'O']\n",
      "['주', 'PR-B']\n",
      "['가', 'PR-I']\n",
      "['는', 'O']\n",
      "[' ', 'O']\n",
      "['얼', 'O']\n",
      "['마', 'O']\n",
      "['야', 'O']\n"
     ]
    }
   ],
   "source": [
    "df_train=df('./data','train.tsv')\n",
    "df_valid=df('./data','valid.tsv')\n",
    "texts_train=df_train[0]\n",
    "text_tokenized = tokenizer(texts_train, padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "labels_train=df_train[1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def text_to_label(text,label):\n",
    "    ttls=[]\n",
    "    for t,l in zip(text,label):\n",
    "        ttl=[t,l]\n",
    "        ttls.append(ttl)\n",
    "    \n",
    "    return ttls\n",
    "     \n",
    "a=text_to_label(texts_train[0], labels_train[0])\n",
    "for i in a:\n",
    "    print(i)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라벨 딕셔너리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " 'O': 1,\n",
       " 'DAT-B': 2,\n",
       " 'DAT-I': 3,\n",
       " 'ST-B': 4,\n",
       " 'ST-I': 5,\n",
       " 'PR-B': 6,\n",
       " 'PR-I': 7}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개체라벨 딕셔너리\n",
    "labels=[label.strip() for label in open(os.path.join('./data', 'label.txt'), 'r', encoding='utf-8')]\n",
    "labels_to_ids = {word:i for i, word in enumerate(labels)}\n",
    "ids_to_labels= {i:word for i, word in enumerate(labels)}\n",
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "label_all_tokens=False\n",
    "def align_label(text, label):\n",
    "    tokenized_input= tokenizer(text, padding='max_length', max_length=32, truncation=True,return_tensors=\"pt\")\n",
    "    label_ids = []\n",
    "    previous_word_idx=None\n",
    "    label_all_tokens=True\n",
    "    j=0\n",
    "    for i in tokenized_input['input_ids'][0]:\n",
    "\n",
    "            \n",
    "        if i== 0:\n",
    "            label_ids.append(-100)\n",
    "        elif i== 2:\n",
    "            label_ids.append(-100)\n",
    "        elif i==3:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            label_ids.append(labels_to_ids[label[j]])\n",
    "            j +=1\n",
    "      \n",
    "\n",
    "    return label_ids\n",
    "\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,df):\n",
    "\n",
    "\n",
    "        self.texts = [tokenizer(i,\n",
    "                               padding='max_length', max_length = 32, truncation=True, return_tensors=\"pt\") for i in df[0]]\n",
    "                               \n",
    "        self.labels = [align_label(i,j) for i,j in zip(df[0], df[1])]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=df('./data','train.tsv')\n",
    "DataSequence(df_train)[0]\n",
    "len([align_label(i,j) for i,j in zip(df_train[0], df_train[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataset = DataSequence(df_train)\n",
    "val_dataset = DataSequence(df_valid)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=4)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertConfig,\n",
    "    DistilBertConfig,\n",
    "    ElectraConfig,\n",
    "    ElectraTokenizer,\n",
    "    BertTokenizer,\n",
    "    BertForTokenClassification,\n",
    "    DistilBertForTokenClassification,\n",
    "    AutoModelForTokenClassification,AutoTokenizer, ElectraForSequenceClassification, AdamW,ElectraModel,\n",
    "    )\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.bert = AutoModelForTokenClassification.from_pretrained(\"monologg/kocharelectra-base-discriminator\", num_labels=len(labels))\n",
    "\n",
    "    def forward(self, input_id, token_type_id,mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, token_type_ids=token_type_id,attention_mask=mask, labels=label, return_dict=False)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습(파인튜닝)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kocharelectra-base-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/kocharelectra-base-discriminator and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/planning/.conda/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 20/20 [00:01<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Loss:  1.850 | Accuracy:  0.429 | Val_Loss:  1.763 | Accuracy:  0.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Loss:  1.484 | Accuracy:  0.698 | Val_Loss:  1.607 | Accuracy:  0.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Loss:  1.385 | Accuracy:  0.577 | Val_Loss:  1.517 | Accuracy:  0.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Loss:  1.111 | Accuracy:  0.672 | Val_Loss:  1.372 | Accuracy:  0.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Loss:  1.142 | Accuracy:  0.626 | Val_Loss:  1.227 | Accuracy:  0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Loss:  1.104 | Accuracy:  0.649 | Val_Loss:  1.018 | Accuracy:  0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Loss:  0.852 | Accuracy:  0.759 | Val_Loss:  0.867 | Accuracy:  0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Loss:  0.791 | Accuracy:  0.749 | Val_Loss:  0.749 | Accuracy:  0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Loss:  0.758 | Accuracy:  0.770 | Val_Loss:  0.660 | Accuracy:  0.902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Loss:  0.545 | Accuracy:  0.850 | Val_Loss:  0.567 | Accuracy:  0.926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Loss:  0.518 | Accuracy:  0.880 | Val_Loss:  0.483 | Accuracy:  0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Loss:  0.532 | Accuracy:  0.884 | Val_Loss:  0.419 | Accuracy:  0.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Loss:  0.465 | Accuracy:  0.914 | Val_Loss:  0.394 | Accuracy:  0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Loss:  0.501 | Accuracy:  0.942 | Val_Loss:  0.327 | Accuracy:  0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Loss:  0.369 | Accuracy:  0.948 | Val_Loss:  0.286 | Accuracy:  0.996\n"
     ]
    }
   ],
   "source": [
    "from torch_optimizer import SGDP\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_loop(model, df_train, df_valid):\n",
    "\n",
    "    train_dataset = DataSequence(df_train)\n",
    "    val_dataset = DataSequence(df_valid)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=16, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    best_acc = 0\n",
    "    best_loss = 0\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            train_label = train_label[0].to(device)\n",
    "            token_type_id= train_data['token_type_ids'][0].to(device)\n",
    "            mask = train_data['attention_mask'][0].to(device)\n",
    "            input_id = train_data['input_ids'][0].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(input_id,token_type_id, mask, train_label)[0]\n",
    "            logits= model(input_id, token_type_id,mask, train_label)[1]\n",
    "\n",
    "            logits_clean = logits[0][train_label != -100]\n",
    "            label_clean = train_label[train_label != -100]\n",
    "\n",
    "            predictions = logits_clean.argmax(dim=1)\n",
    "\n",
    "            acc = (predictions == label_clean).float().mean()\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        for val_data, val_label in val_dataloader:\n",
    "\n",
    "            val_label = val_label[0].to(device)\n",
    "            token_type_id= train_data['token_type_ids'][0].to(device)\n",
    "            mask = val_data['attention_mask'][0].to(device)\n",
    "            input_id = val_data['input_ids'][0].to(device)\n",
    "\n",
    "            loss= model(input_id,token_type_id, mask, val_label)[0]\n",
    "            logits=model(input_id,token_type_id, mask, val_label)[1]\n",
    "\n",
    "            logits_clean = logits[0][val_label != -100]\n",
    "            label_clean = val_label[val_label != -100]\n",
    "\n",
    "            predictions = logits_clean.argmax(dim=1)          \n",
    "\n",
    "            acc = (predictions == label_clean).float().mean()\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += loss.item()\n",
    "\n",
    "        val_accuracy = total_acc_val / len(val_dataloader)\n",
    "        val_loss = total_loss_val / len(val_dataloader)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(train_dataloader): .3f} | Accuracy: {total_acc_train / len(train_dataloader): .3f} | Val_Loss: {total_loss_val / len(val_dataloader): .3f} | Accuracy: {total_acc_val / len(val_dataloader): .3f}')\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 15\n",
    "model=Model()\n",
    "train_loop(model, df_train, df_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "label: tensor([4, 5, 5, 5, 1, 6, 7, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor([4, 5, 5, 5, 1, 6, 7, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "label: tensor([4, 5, 5, 5, 5, 5, 5, 1, 6, 7, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor([4, 5, 5, 5, 5, 5, 5, 1, 6, 7, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "label: tensor([1, 1, 1, 6, 7, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "label: tensor([4, 5, 5, 5, 1, 6, 7, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor([4, 5, 5, 5, 1, 6, 7, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "label: tensor([4, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor([4, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "label: tensor([2, 3, 1, 4, 5, 5, 5, 1, 6, 7, 1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor([2, 3, 1, 4, 5, 5, 5, 1, 6, 7, 1, 1, 1, 1], device='cuda:0')\n",
      "label: tensor([4, 5, 5, 5, 1, 6, 7, 1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor([4, 5, 5, 5, 1, 6, 7, 1, 1, 1, 1], device='cuda:0')\n",
      "label: tensor([4, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor([4, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "label: tensor([1, 1, 1, 1, 1, 1, 1, 4, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "Test Accuracy:  0.958\n"
     ]
    }
   ],
   "source": [
    "df_test=df('./data','test.tsv')\n",
    "test_dataset = DataSequence(df_test)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n",
    "\n",
    "for train_data, train_label in test_dataloader:\n",
    "    train_label = train_label[0].to(device)\n",
    "    token_type_id= train_data['token_type_ids'][0].to(device)\n",
    "    mask = train_data['attention_mask'][0].to(device)\n",
    "    input_id = train_data['input_ids'][0].to(device)\n",
    "    print(model(input_id,token_type_id, mask, train_label)[0])\n",
    "    logits_clean=model(input_id,token_type_id, mask, train_label)[1][0][train_label != -100]\n",
    "   \n",
    "df_test=df('./data','test.tsv')\n",
    "def evaluate(model, df_test):\n",
    "\n",
    "    test_dataset = DataSequence(df_test)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0.0\n",
    "    total_cor=0.0\n",
    "    total_n=0.0\n",
    "\n",
    "    for test_data, test_label in test_dataloader:\n",
    "\n",
    "        test_label = test_label[0].to(device)\n",
    "        token_type_id= train_data['token_type_ids'][0].to(device)\n",
    "        mask = test_data['attention_mask'][0].to(device)\n",
    "        input_id = test_data['input_ids'][0].to(device)\n",
    "          \n",
    "        loss= model(input_id, token_type_id,mask, test_label.long())[0]\n",
    "        logits= model(input_id, token_type_id,mask, test_label.long())[1]\n",
    "\n",
    "        logits_clean = logits[0][test_label != -100]\n",
    "        label_clean = test_label[test_label != -100]\n",
    "\n",
    "        predictions = logits_clean.argmax(dim=1)\n",
    "        acc= (predictions == label_clean).float().mean()    \n",
    "        print(\"label:\", label_clean)\n",
    "        print(\"pred:\",predictions)\n",
    "        total_acc_test += acc\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_dataloader): .3f}')\n",
    "\n",
    "evaluate(model, df_test)\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 삼성죤자 주가 좀 알려줘\n",
      "['DAT-B', 'DAT-I', 'O', 'ST-B', 'ST-I', 'ST-I', 'ST-I', 'O', 'PR-B', 'PR-I', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "def align_word_ids(text):\n",
    "    tokenized_input= tokenizer(text, padding='max_length', max_length=32, truncation=True,return_tensors=\"pt\")\n",
    "    label_ids = []\n",
    "    previous_word_idx=None\n",
    "    label_all_tokens=True\n",
    "    j=0\n",
    "    for i in tokenized_input['input_ids'][0]:\n",
    "\n",
    "            \n",
    "        if i== 0:\n",
    "            label_ids.append(-100)\n",
    "        elif i== 2:\n",
    "            label_ids.append(-100)\n",
    "        elif i==3:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            label_ids.append(1)\n",
    "            j +=1\n",
    "      \n",
    "\n",
    "    return label_ids\n",
    "\n",
    "def evaluate_one_text(model, sentence):\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = 32, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    mask = text['attention_mask'][0].unsqueeze(0).to(device)\n",
    "    token_type_id= train_data['token_type_ids'][0].to(device)\n",
    "    input_id = text['input_ids'][0].unsqueeze(0).to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(input_id,token_type_id, mask, None)\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "\n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
    "    print(sentence)\n",
    "    print(prediction_label)\n",
    "            \n",
    "evaluate_one_text(model, '오늘 삼성죤자 주가 좀 알려줘')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
